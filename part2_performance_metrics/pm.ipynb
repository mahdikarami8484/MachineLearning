{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d969158",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1, 1, 0, 0, 1, 0, 1, 0, 0, 0]\n",
    "y_pred = [1, 0, 1, 0, 1, 0, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acbc1848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "acc*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e730316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "pr = precision_score(y_true, y_pred)\n",
    "pr*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "737f5d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "r = recall_score(y_true, y_pred)\n",
    "r*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af39a2",
   "metadata": {},
   "source": [
    "حتماً 🙂 بزار خیلی ساده و تصویری برات توضیح بدم:\n",
    "\n",
    "ما معمولاً **precision** و **recall** رو وقتی محاسبه می‌کنیم که یه **کلاس مثبت** (مثلاً بیمار، اسپم، تقلب، …) داریم و مدل پیش‌بینی می‌کنه.\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Precision (دقت)\n",
    "\n",
    "**سوال:** از بین همه نمونه‌هایی که مدل گفت “مثبت” هستند، چند درصد واقعاً مثبت بودن؟\n",
    "\n",
    "فرمول:\n",
    "[\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "]\n",
    "\n",
    "* TP = True Positive → درست پیش‌بینی شده مثبت\n",
    "* FP = False Positive → اشتباه پیش‌بینی شده مثبت\n",
    "\n",
    "**مثال تصویری:**\n",
    "\n",
    "* مدل میگه 10 تا ایمیل اسپم هست\n",
    "* ولی فقط 7 تا واقعاً اسپم بودن\n",
    "* precision = 7 / 10 = 0.7\n",
    "\n",
    "> یعنی دقت پیش‌بینی مثبت‌ها ۷۰٪ است.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Recall (بازخوانی / حساسیت)\n",
    "\n",
    "**سوال:** از بین همه نمونه‌های واقعا مثبت، مدل چند درصدشون رو درست تشخیص داده؟\n",
    "\n",
    "فرمول:\n",
    "[\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "]\n",
    "\n",
    "* FN = False Negative → اشتباه پیش‌بینی شده منفی\n",
    "\n",
    "**مثال تصویری:**\n",
    "\n",
    "* در کل 15 ایمیل واقعاً اسپم بودن\n",
    "* مدل فقط 7 تا رو تشخیص داده\n",
    "* recall = 7 / 15 ≈ 0.467\n",
    "\n",
    "> یعنی مدل فقط ۴۶٪ از اسپم‌ها رو پیدا کرده.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚡ تفاوت کلیدی\n",
    "\n",
    "* **Precision:** وقتی میگیم مثبت، چقدر مطمئن هستیم\n",
    "* **Recall:** از همه مثبت‌ها، چقدر رو پیدا کردیم\n",
    "\n",
    "💡 ساده‌تر:\n",
    "\n",
    "* Precision = «وقتی می‌گم اسپم، درست گفتم؟»\n",
    "* Recall = «چند درصد اسپم‌ها رو تونستم پیدا کنم؟»\n",
    "\n",
    "---\n",
    "\n",
    "اگر بخوای، می‌تونم یه **نمودار ساده با مربع‌ها و دایره‌ها** بکشم که فرق **precision و recall** رو خیلی واضح نشون بده، تا یادت بمونه.\n",
    "\n",
    "میخوای برات بکشم؟\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
